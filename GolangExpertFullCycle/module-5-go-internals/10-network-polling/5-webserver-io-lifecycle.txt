=====================================
  GO EM AÇÃO: O CICLO DE I/O DE UM WEBSERVER
=====================================
- O funcionamento de um servidor web em Go é a demonstração prática e final de como os componentes do runtime (scheduler, netpoller) e os mecanismos do SO (FDs, epoll) trabalham juntos para alcançar alta performance e concorrência.

-------------------------------------
  OS DOIS TIPOS DE SOCKET: "PORTEIRO" E "GARÇOM"
-------------------------------------
- Para entender o fluxo, é crucial diferenciar os dois tipos de sockets/FDs envolvidos:

  1. LISTENING SOCKET (O "PORTEIRO"):
     - Quando você inicia um servidor com `http.ListenAndServe(":8080", nil)`, o Go cria UM socket principal que fica "escutando" na porta 8080.
     - O único trabalho deste FD é esperar por novas tentativas de conexão. Ele não transfere dados de requisições (HTML, JSON, etc.).

  2. CONNECTION SOCKET (O "GARÇOM"):
     - Toda vez que o "Porteiro" aceita uma nova conexão de um cliente, o Kernel cria um NOVO socket, com um NOVO FD, exclusivamente para a comunicação com aquele cliente.
     - É através deste FD que os dados da requisição HTTP são lidos e as respostas são escritas. Cada cliente conectado tem seu próprio "Garçom".

-------------------------------------
  O CICLO DE VIDA DE UMA REQUISIÇÃO (PASSO A PASSO)
-------------------------------------

  == FASE 1: INICIALIZAÇÃO DO SERVIDOR ==

  1. `http.ListenAndServe` é chamado.
  2. O Go pede ao Kernel para criar o **Listening Socket** na porta 8080. O Kernel cria o socket e retorna seu **FD_Porteiro**.
  3. O runtime do Go, através do Netpoller, registra o **FD_Porteiro** na instância do `epoll` com a instrução: "Me avise quando este FD estiver pronto para LEITURA" (pronto para `accept`).
  4. O Netpoller chama `epoll_wait` e a thread principal do poller bloqueia eficientemente, sem consumir CPU, esperando por eventos.

  == FASE 2: CHEGADA DE UMA NOVA CONEXÃO ==

  5. Um cliente (ex: um navegador) tenta se conectar ao `localhost:8080`.
  6. O Kernel recebe a solicitação e marca o **FD_Porteiro** como "pronto".
  7. A chamada `epoll_wait` do Netpoller retorna, notificando que o **FD_Porteiro** tem um evento.
  8. O Netpoller avisa o scheduler do Go. O runtime aceita a conexão (`accept`).

  == FASE 3: MANIPULAÇÃO DA CONEXÃO ==

  9. A chamada `accept` faz com que o Kernel crie um **Connection Socket** novinho em folha, com seu próprio **FD_Garçom**.
  10. O pacote `net/http` do Go inicia uma **nova goroutine** para lidar com este cliente específico.
  11. Dentro dessa nova goroutine, o **FD_Garçom** é registrado no `epoll` com a instrução: "Me avise quando este novo FD estiver pronto para LEITURA" (esperando os dados da requisição HTTP).
  12. A goroutine do cliente é "estacionada" e a thread do SO fica livre.

  == FASE 4: PROCESSAMENTO DA REQUISIÇÃO ==

  13. O cliente envia a requisição HTTP (ex: `GET /home`).
  14. O Kernel recebe os dados e marca o **FD_Garçom** como "pronto".
  15. `epoll_wait` retorna, notificando o Netpoller sobre o evento no **FD_Garçom**.
  16. O Netpoller "acorda" a goroutine específica daquele cliente.
  17. A goroutine lê os dados da requisição, executa o handler, escreve a resposta de volta no **FD_Garçom** e, finalmente, fecha a conexão.

-------------------------------------
  OTIMIZAÇÃO: PROCESSAMENTO EM LOTE (BATCH) DE EVENTOS
-------------------------------------
- As suas considerações finais são um ponto chave da performance do Go.
- Go não chama `epoll_wait` para receber apenas um evento por vez.
- A chamada `epoll_wait` é configurada para receber um **lote (batch) de eventos**. Se 100 clientes enviarem dados ao mesmo tempo, uma única chamada ao `epoll_wait` pode retornar uma lista com os 100 FDs prontos.
- O Netpoller então percorre essa lista e acorda as 100 goroutines correspondentes de uma só vez.
- Isso reduz drasticamente o número de chamadas de sistema (syscalls), que são operações custosas, permitindo que o Go atinja uma vazão (throughput) muito alta.

-------------------------------------
  GO x NODE.JS (COMPARAÇÃO FINAL)
-------------------------------------
- O ciclo de vida de I/O em um servidor Node.js é conceitualmente idêntico. A `libuv` cria um "listening socket", o registra no `epoll`, e usa um loop de eventos para aceitar novas conexões e ler dados.
- A GRANDE DIFERENÇA ARQUITETURAL:
  * Em Node.js, quando uma nova requisição chega, seu código de manipulação (o `handler`) é executado como um callback ou uma Promise na **thread única do Event Loop**. A concorrência é gerenciada pelo Event Loop, mas não há paralelismo real para o código JavaScript.
  * Em Go, para cada nova conexão, uma **nova goroutine** é criada. Graças ao scheduler M:N, essas goroutines podem ser distribuídas entre múltiplos núcleos de CPU e rodar em **paralelo real**. Isso torna o Go naturalmente adequado para aproveitar hardware moderno em tarefas de CPU-bound que podem ocorrer durante o processamento de uma requisição.

-------------------------------------
  RESUMO FINAL DO MÓDULO
-------------------------------------
✅ Goroutines leves permitem um modelo de "uma goroutine por conexão".
✅ O Scheduler M:N distribui essas goroutines em threads do SO, permitindo paralelismo real.
✅ File Descriptors (FDs) são a interface do SO para I/O.
✅ O Netpoller usa `epoll`/`kqueue` para monitorar milhares de FDs eficientemente.
✅ Quando uma goroutine faz I/O, ela "dorme", liberando sua thread, e o Netpoller a "acorda" quando o trabalho está pronto.
✅ O resultado é um servidor de alta performance, concorrente e que utiliza os recursos da CPU de forma eficiente, tudo abstraído de forma elegante para o desenvolvedor.